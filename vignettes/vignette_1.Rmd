---
title: "Survival Curve Extrapolation with easysurv"
author: Niall Davison & Brad Kievit
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Survival Curve Extrapolation with easysurv}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


# Introduction

Welcome! The `easysurv` package was created by and for the easy Health Group. It's comprised of a set of functions that can be leveraged to turn individual patient data (IPD) or pseudo-IPD into a set of modeled parametric curves. It can also organize the data from these modeled curves to be exported into an Excel workbook.

The primary function of easysurv is to support the estimation and inspection of standard parametric survival extrapolations. `easysurv` can also assist in mixture cure and spline modelling, but these are not explained in detail in this vignette.

In this vignette, we'll go through the motions of taking survival data, cleaning and processing it, fitting a set of distributions via standard parametric modeling methods, and exporting outputs to Excel. The survival data used in this vignette comes from the `lung` dataset, which is a part of the `survival` package.

Once the `easysurv` package is downloaded, anyone can follow along by accessing the `demo` folder within the `easysurv` package folder. The demonstration in this vignette can be performed in one step by running the `two_arm.R` script. The code in the example `two_arm.R` script cycles through scripts in the `two_arm_workflow` folder.

These scripts are numbered sequentially in the order of use:

* 1_data_setup.R
* 2_kaplan_meier.R
* 3_tests.R
* 4a_fitting_standard.R
* 5a_extract_standard.R
* 6_output.standard.R

We’ll give directions on which script should be accessed and when as we walk through the exercise.

<br>

# Starting the analysis
##### Demo script: two_arm.R

To begin, we'll work from the overview script, `two_arm.R`, which can be found in the `demo` folder. We'll start with a clean environment so that no other pre-existing R objects or functions will interfere with what we want to do. We'll then load the packages we require. `easysurv` is this package and `here` is used to orient the working directory to our current project. We'll then set our current project to refer to the demo described in this vignette. We'll also adjust the scientific notation formatting using `scipen`.

```{r, warning=FALSE, message=FALSE, fig.show='hide'}
# Start from a clean environment
rm(list = ls())

library(easysurv) # will auto-load dplyr, survival, ggplot2
library(here)

options(scipen = 999) # suppress scientific notation

directory <- here::here("inst/extdata/two_arm_workflow")
```

Next, we need to decide which analyses we want to complete. We have the option to fit a standard parametric survival curve model, a mixture cure model and a flexible spline model. The corresponding scripts of the `demo/two_arm_workflow` will conditionally run based on the true/false status of a set of indicators. Of note, these are specific to this demo workflow and aren't necessary for someone to use `easysurv` outside of this context.

We can choose to complete as many or as few of these analyses as we want. Here, we'll choose to complete just the standard parametric fitting.

```{r, warning=FALSE, message=FALSE}
## Choose Analyses ------------------------------------------------------------
do_standard <- TRUE # TRUE: run standard parametric survival model fits
do_cure <- FALSE # TRUE: run mixture cure model fits
do_splines <- FALSE # TRUE: run spline model fits
```

The outputs of analyses performed using the `easysurv` package can be exported to Excel using the `openxlsx` package.. In our demo, we have included a toggle to determine whether we export results to Excel. Here, we’ll choose to enable it.

```{r}
output_to_excel <- TRUE # TRUE: store outputs into a template Excel file
```

Note that, when this Excel file is created, it will automatically be saved to the "Downloads" folder of our local system.

This package has been set up to use the Roboto Condensed font. However, this font can only be accessed if it has been installed onto the machine we're using. Since this may be more difficult for some machines, there is an option to simply use the stock font. In our case, since it's already been set up, we'll elect to use this font.

```{r}
use_package_font <- TRUE # TRUE: use Roboto Condensed font in plots
```

The remainder of the analysis in the `two_arm.R` file is structured so that we can run one module at a time by simply using `source()` on an appropriate script. These modules are ordered as follows: Data setup, KM analysis, Testing, Fitting models, Extracting summaries, and Exporting to Excel. Notably, the latter three modules each have three distinct scripts for the different types of analyses (i.e., standard, cure, splines). If we wanted to keep things simple, we could simply set up our dataset in the KM analysis script, then click through each of these `source()` operations. This time, we'll go into each script pertaining to the standard parametric method to explore what they do.

<br>

# Data Setup
##### Demo script: 1_data_setup.R

The first step in the proper analysis is importing and setting up our dataset. The `easysurv` package does not contain functions to clean or import data, other more established packages are used here.

The data setup for our demo is laid out in `demo/two_arm_workflow/1_data_setup.R`. As mentioned previously, we are going to be working with the lung dataset from the survival package. If we wanted to work with a different data set, this is where we would supply it. We’ll first import this data into our session and take a look at a snippet of it.


```{r}
# Import your data here
input_data <- survival::lung # 'lung' taken from 'survival' package for testing.

# Assess input_data
head(input_data, 6) # Looks at first 6 rows.
```

`lung` has a few different variables but, because we're survival modelling, we're most interested in the `time` variable, and the event variables, which is labeled `status`. We want to assess differences in survival between men and women so we're also interested in the `sex` variable.

To clean this data up, we'll make a new data frame and call it `surv_data`:

* The `time` variable is in days and we'd like our analysis output to be in months, so we'll divide this variable by the average number of days in a month.
* Our event variable `status` is coded as "1" or "2", but survival data using the `survival` package in R uses "0" when individuals are right-censored and "1" when they experience the event of interest, which is what we want here. Otherwise, a "2" would be interpreted as a left-censor. To fix this we can simply subtract "1" from every value in the variable.
* Our `sex` variable is coded numerically. In order for R to recognize this variable as discrete, we need to use the `as.factor()` function on it.

Now we've cleaned our data and selected the variables of interest.

```{r}
# Create survival data.frame (surv_data)
## 'time' (numeric) = time of event/censor.
## 'event' (numeric)  = status [0 = right censored] [1 = event at 'time'].
## 'treatment' (factor) = treatment received.
surv_data <- data.frame(
  time = input_data$time,
  event = input_data$status - 1,
  treatment = as.factor(input_data$sex)
)

# Assess surv_data
head(surv_data)
```

Let's see how many individuals we have in each treatment arm (i.e. sex).

```{r}
# Tally # of individuals in each treatment arm
library(dplyr)

surv_data %>%
  group_by(treatment) %>%
  tally()
```

We have 90 females (treatment = 2) and 138 males (treatment = 1). If we know the number of individuals in each treatment arm already, this is a good check to make sure the totals are the same as we expected.

Let's quickly look at a summary of our data. How many events and censors occur for each sex?

```{r}
# Assess event/censor counts for each treatment
surv_data %>%
  group_by(event, treatment) %>%
  tally()
```

It appears females experience less death events (n = 53) than males (n = 112). It seems like our covariate might be an important indicator of survival, but we'll find that out later.

For the sake of this example, we’ll assume that females are our “intervention” arm and males are our “comparator”. These variables will remain numbers in our code, but can be named whatever we want for nicer output. Here, we'll just call them "My intervention" and "My comparator" respectively. To make sure we're looking at the effect of our "intervention" and not the other way around, we can also re-level the dataset.

```{r}
# Define the names of the treatments of interest, as they appear in input_data.
name_intv <- "2" # Female
name_comp <- "1" # Male

# Names of treatments used in plots etc.
name_intv_nice <- "My intervention" # Female
name_comp_nice <- "My comparator" # Male

# Set reference treatment.
surv_data <- within(surv_data, treatment <- relevel(treatment, 
                                                    ref = paste(name_comp)))
```

For some analyses, we’ll want to fit survival curves for the whole data set and use the treatment as a covariate in the estimation. Other times, we’ll be interested in survival of a specific arm irrespective of the other treatment arm, so we’ll want to fit survival curves separately. To do the latter, we need the option to work with data sets that only contain one arm or the other.

```{r}
# Split data into two treatments
surv_data_intv <- surv_data[surv_data$treatment == paste(name_intv), ]
surv_data_comp <- surv_data[surv_data$treatment == paste(name_comp), ]
```

Next, we’ll set up the time horizon and time increments of the plots and tabular outputs. The estimated survival parameters themselves won’t be restricted by these inputs.

* `max_time` specifies how far in the future we want to go. Here, we'll go 2.5x longer than the longest survival time supplied by the dataset. Note that we could also just supply a hard-coded number for `max_time` if needed.
* `times` specifies the interval we want between each time in the trace. 
    + Here, we'll use the `seq()` function to simply specify that we want to go from t = 0 months out to the max time we just specified, but we want to generate 200 equally spaced data points. The `seq()` function will determine the appropriate interval to make that work.
        - Notably, if we didn't care about how many data points get generated, we could use the `by = ` argument instead of `length.out` to specify the time between intervals.

```{r}
# Define variables for use in plots
max_time <- ceiling(max(surv_data$time) * 2.5) # Define the max time for plots
times <- seq(from = 0, to = max_time, length.out = 200)
```

We'll wrap up the data setup by providing a few more label names.
```{r}
# Define variables for Excel output
population <- "ITT"
study <- "Study name"
time_unit <- "Months"

# Define one more variable for use in plots
endpoint <- "Overall Survival"
```

Next up, we'll turn our newly cleaned dataset into Kaplan-Meier data so that we can properly plot it. To do this, we'll move over to the `2_kaplan_meier.R` script, located in the same `two_arm_workflow` folder.

<br>

# Kaplan-Meier curves
##### Demo script: 2_kaplan_meier.R

Right now our data is just a data frame. We want to turn it into a survival curve than can be plotted and can be recognized by other survival analysis functions. To do this, we'll first use the `survfit` & `Surv` functions from the `survival` package. 

```{r}
KM_est <- survival::survfit(survival::Surv(time, event) ~ treatment,
  conf.int = 0.95,
  data = surv_data,
  type = "kaplan-meier"
)
```

`Surv` creates a survival object that can be used as the dependent variable in a Kaplan-Meier formula. Here, we are looking at the effect of `treatment` on our `event` column over `time`. `survfit` turns this survival object into a curve. Note that we also supply a confidence interval, the type of curve we want to see (which in the case is `"kaplan-meier"`), and the source of the data to be used.

Now that we have a survival curve, we can plot it. This is the first time in the demo that we will use a function from the `easysurv` package. The `plot_KM` function can be used to plot Kaplan-Meier `survfit` objects using a easy aesthetic.

```{r}
KM.plot <- easysurv::plot_KM(
  fit = KM_est,
  legend.labs = c(
    name_comp_nice,
    name_intv_nice
  ),
  xlim = c(
    0,
    max(surv_data$time) * 1.1
  ),
  title = "Kaplan-Meier plot",
  subtitle = endpoint
)
```

Notice that, in addition to asking for the `survfit` object, we can also supply the range of the time axis (`xlim`), a graph title, and a graph subtitle. We can choose to keep the easy aesthetic by assigning `use_package_theme` to `T` or `TRUE`. Earlier in the demo we set `use_package_font` to `TRUE`, since we have the Roboto Condensed font installed and loaded, so we can also use this font in the plot.

Let's see how it looks:

```{r, fig.width=6, fig.height=5, warning = F}
print(KM.plot)
```

This further confirms our suspicion that there may be a treatment effect, especially considering the difference in median survival probability, as denoted by the dotted lines.

The `summarise_KM` function in the `easysurv` package can be used to summarise the KM object, so that we don’t just have to assess this data graphically. 

```{r}
# Medians and 95% confidence limits
KM_summary <- easysurv::summarise_KM(KM_est)
print(KM_summary, width = 200)
```

To wrap up the Kaplan-Meier portion of this analysis, we'll also create `survfit` objects for each of the treatments on their own. There isn't too much to use these with for now, but we'll export these objects into the final Excel sheet as simple Kaplan-Meier tables later, so we'll just create those now as well using the `step_KM` function. This `easysurv` function processes Kaplan-Meier data by duplicating each row and then assigning the survival values from the previous row to these new duplicated rows. This enables a conventional "stepped" aesthetic when plotting the Kaplan-Meier data in Excel. 

```{r}
KM_est_intv <- survival::survfit(
  survival::Surv(time, event) ~ 1, 
  conf.int = 0.95, data = surv_data_intv, type = "kaplan-meier")
KM_est_comp <- survival::survfit(survival::Surv(time, event) ~ 1, 
                                 conf.int = 0.95, data = surv_data_comp, 
                                 type = "kaplan-meier")

## Stepped KMs
KM_output_intv <- easysurv::step_KM(KM = KM_est_intv)
KM_output_comp <- easysurv::step_KM(KM = KM_est_comp)
```

From here, we'll use our new `survfit` object to run some diagnostic tests, which will be informative for future curve fitting. To do this, we'll move over to the `3_tests.R` script, located in the same `two_arm_workflow` folder.

<br>

# Diagnostic tests
##### Demo script: 3_tests.R

The first thing we'll do is create a log cumulative hazard plot. This will allow us to visually assess whether our two treatments satisfy the proportional hazards assumption for joint modelling. We'll use the same `plot_KM` function as before, but will supply an additional argument of `fun = "cloglog"`.

```{r, fig.width=6, fig.height=5, warning = F}
cloglog.plot <- easysurv::plot_KM(
  fit = KM_est,
  fun = "cloglog",
  legend.labs = c(
    name_comp_nice,
    name_intv_nice
  ),
  title = "Log cumulative hazard plot",
  subtitle = endpoint,
  surv.median.line = "none"
)

print(cloglog.plot)
```

If the lines appear mostly parallel, then the proportional hazards assumption is most likely met. This seems to be the case here, but we also want to test this statistically. To do this, we'll first fit our data to a Cox regression model using `coxph` from the `survival` package. We'll save the output of this model as an object, `KM_coxph` and feed it into the function for a Schoenfeld test, which tests whether the covariates of the model have proportional hazards. The function for this is `cox.zph`. The `table` element of this `cox.zph` object can be called by tagging `$table` onto the named `cox.zph` object. This table shows the p-value for the proportional hazards test, which we'll display as a data frame.

```{r}
fit_both <- survival::Surv(time, event) ~ as.factor(treatment)
KM_coxph <- survival::coxph(formula = fit_both, data = surv_data)

summary_coxph <- survival::cox.zph(KM_coxph)
summary_coxph <- as.data.frame(summary_coxph$table)

summary_coxph
```

Note that the `coxph` function requires our data to be in the form of a `Surv` object.

Our p-value is 0.09, which is greater than 0.05, so we do not need to reject the proportional hazards assumption. It is however quite close to the significance level, so caution should be taken.

Another plot of interest is the Schoenfeld plot, which visually assesses the statistical test from before. Putting it simply, the plot is an indicator of the hazard ratio drift over time. If the proportional hazards assumption is true, we want it to approximate a horizontal line along y = 0. We can create this plot with an easysurv aesthetic using a customized `ggcoxdiagnostics` function.  

```{r, fig.width=6, fig.height=5, warning = F}
library(ggplot2)
schoenfeld.plot <-
  easysurv::plot_schoenfeld(
    fit = KM_coxph,
    formula = fit_both,
    data = surv_data,
    title = "Schoenfeld residuals",
    subtitle = endpoint
  ) +
  theme(strip.text.x = element_blank())

print(schoenfeld.plot)
```

Since our confidence bands contain the horizontal line at y = 0 (red dotted line), we do not need to reject the proportional hazards assumptions. Having said that, the fitted curve (blue dotted line) does appear to be steadily increasing, indicating the the hazard ratio may not be consistent. As with the above tests, the proportional hazards assumption should be taken with a measure of caution.

Now that we have a good idea of where we stand statistically, we can move on to actually fitting our survival data to some curves. To do this, we'll move over to the `4a_fitting_standard.R` script, located in the same `two_arm_workflow` folder. Note that, if we were working with a cure model or wanted to fit the data to splines, we could instead go to `4b_fitting_cure.R` or `4c_fitting_splines.R` respectively.

<br>

# Fitting Survival Curves
##### Demo script: two_arm.R, 4a_fitting_standard.R

Before we can start fitting any curves, we need to decide off the top if we would like to fit our covariate effects jointly or individually. Based on what we learned from the statistical and visual tests above, it appears that our covariate may have proportional hazards, so we are permitted to use a joint fit. With that being said, the p-value wasn't as high as we'd like (p = 0.09) and the Schoenfeld plot showed some indications of a moving hazard ratio so, we're also going to look at individually fitted curves.

In our demo, we include a `fit_type` variable in the `two_arm.R` script, which can be `"individual"`, `"joint"`, or `c("individual", "joint")`. This allows us to conditionally control the type of fits explored. Of note, these are specific to this demo workflow and aren't necessary for someone to use `easysurv` outside of this context.

```{r}
# Select fit type below 
# Base on statistical test results (plots and message in console)

# Options: "individual", "joint", or c("individual", "joint")
fit_type <- c("individual", "joint") 
```

In the demo script for standard parametric survival models (`4a_fitting_standard.R`) we start by deciding which distributions we want to assess. We can choose as many or as few as we want, so long as it’s available as a dist option in `flexsurvreg`, which is the underlying function of our custom fitting function, `fit.models`. Here, we’re going to fit curves to “Exponential”, “Gamma”, “Generalized Gamma”, “Gompertz”, “log-logistic”, “log-normal” and “Weibull” distributions. Except for gamma, these are the distributions recommended by NICE in Technical Support Document (TSD) 14, ["Survival analysis: Extrapolating patient data"](https://www.sheffield.ac.uk/media/34225/download?attachment). Gamma has been included because it has been required by other HTA bodies, including CADTH. The notation for these distributions is defined per `flexsurvreg`.

```{r}
# Set a global set of distributions to test.
dists <- c(
  "exp",
  "gamma",
  "gengamma",
  "gompertz",
  "llogis",
  "lnorm",
  "weibull"
)
```

Next, we set up our `Surv` formula. Let’s first look at fitting our covariates individually and we’ll fit them jointly later. The `Surv` formula for this will therefore have no independent covariates, so will be entered as `~ 1`.

We’ll also explicitly state the we want the distribution list to be the same for both the intervention and comparator arms. If we wanted to fit a different set of distributions to either of these, we have that option as well.

```{r}
# Define flexsurv formula (~ 1 means a model with no covariates)
## fit_separate is the primary formula used throughout the code.
fit_separate <- survival::Surv(time, event) ~ 1

### Testing distributions -----------------------------------------------------
dists_intv <- dists
dists_comp <- dists
```

The underlying process of fitting curves to survival data is based around optimizing the maximum likelihood calculated between the expected and observed values, and arriving at a set of coefficients that are specific to each distribution. Sometimes, especially if the survival data is sparse or erratic, the optimization process won't be able to "converge", or confidently determine the best coefficient values for the survival data. Normally when this happens, the function would stop and show an error message. This would break up the flow of our analysis if we were to run the whole thing at once. To rectify this, the `check_converged` function can fit each curve in a context that allows errors to occur without stopping the whole script. If `check_converged` finds any curves for which the coefficients don't converge nicely, it will let us know and will proceed to remove it from the appropriate `dists` list, so that it won't affect our proper curve fitting operations later on.

```{r}
dists_intv <- easysurv::check_converged(
  formula = fit_separate,
  data = surv_data_intv,
  dists = dists_intv
)


dists_comp <- easysurv::check_converged(
  formula = fit_separate,
  data = surv_data_comp,
  dists = dists_comp
)
```

Notice that we supply a `Surv` formula with no covariates (i.e. "~ 1") but only feed in one arm of the survival data at a time.

Since we didn't receive any messages from the above operations, that means all our distributions converged successfully in both arms.

With this in mind, we can now move on to actually fitting our survival curves using our "safety-checked" `dists` lists. To do this, we'll use the `fit.models` function (from the `survHE` package) and supply the same inputs as for `check_converged` above, with the addition of supplying our `method`. As mentioned, for this analysis we want to fit our models using maximum likelihood estimation (i.e. `mle`).

```{r}
# Only fit the distributions that converged.
model_fits_intv <- survHE::fit.models(
  formula = fit_separate,
  data = surv_data_intv,
  distr = dists_intv,
  method = "mle"
)

model_fits_comp <- survHE::fit.models(
  formula = fit_separate,
  data = surv_data_comp,
  distr = dists_comp,
  method = "mle"
)
```

If there were no errors in the fitting process, we can now plot these curves and check them out. We'll do this using the `plot_fits` function from `easysurv`. We'll specify a custom title and subtitle, feed in the time points we assigned previously, and specify whether we want to use the easy theme and/or Roboto Condensed font.

```{r, fig.width=6, fig.height=5}
fits.plot_intv <- easysurv::plot_fits(
  models = model_fits_intv,
  title = paste0(
    "Standard parametric models: ",
    name_intv_nice
  ),
  subtitle = endpoint,
  t = times
)

fits.plot_comp <- easysurv::plot_fits(
  models = model_fits_comp,
  title = paste0(
    "Standard parametric models: ",
    name_comp_nice
  ),
  subtitle = endpoint,
  t = times
)

print(fits.plot_intv)
print(fits.plot_comp)
```

It seems like, though some distributions appear to fit better than others, they all fit pretty nicely.

Often times, we may also wish to visualize the hazards of our curves. This is because, given that the covariates are fitted individually, they have the freedom to change independently over time. We can create these plots using the `plot_smoothed_hazards` function. This function takes all the same inputs as `plot_fits`, but we'll also tell it to keep the upper Y-axis limit to "1". This way, if any hazards get really big, we won't lose the resolution of the ones that don't. Hazard rates tend to jump around a lot, so it's helpful to apply a smoother to the curves, which this function does. That way we can more easily see trends and changes.

```{r, fig.width=6, fig.height=5}
haz.plot_intv <- easysurv::plot_smoothed_hazards(
  data = surv_data_intv,
  fits = model_fits_intv,
  time = "time",
  event = "event",
  t = times,
  title = paste0(
    "Smoothed hazards: ",
    name_intv_nice
  ),
  subtitle = endpoint
)

haz.plot_comp <- easysurv::plot_smoothed_hazards(
  data = surv_data_comp,
  fits = model_fits_comp,
  time = "time",
  event = "event",
  t = times,
  title = paste0(
    "Smoothed hazards: ",
    name_comp_nice
  ),
  subtitle = endpoint
)

print(haz.plot_intv)
print(haz.plot_comp)
```

We can see that some hazards fall within the confidence bands of the source data, while some don't, indicating better or worse fits. It also looks like it was a good call setting the Y-axis limit to "1", as the "Gompertz" hazard got really big for our intervention arm.

Before moving onto the next step in the analysis, we also want to fit and visualize these curves jointly. The process is the same as for individual fits, but this time our `Surv` formula will specify our covariate.

```{r}
# Define flexsurv formula
## fit_separate is the primary formula used throughout the code.
## fit_both is only used in the standard parametric models.
fit_both <- survival::Surv(time, event) ~ as.factor(treatment)
```

We'll also feed the entire data set into each function, instead just one arm at a time, as you can see when we check for curve convergence, then fit and plot the curves.

```{r, fig.width=6, fig.height=5}
dists_both <- dists

dists_both <- easysurv::check_converged(
  formula = fit_both,
  data = surv_data,
  dists = dists_both
)

# Only fit the distributions that converged.
model_fits_both <- survHE::fit.models(
  formula = fit_both,
  data = surv_data,
  distr = dists_both,
  method = "mle"
)

fits.plot_both <- easysurv::plot_fits(
  models = model_fits_both,
  title = "Standard parametric models: jointly fitted",
  subtitle = endpoint,
  t = times
)

print(fits.plot_both)
```

Now we can see both treatment arms together, with one having solid lines and the other having dotted lines.

We now have a good set of R objects and plots to visualize our survival curves! The last part of our analysis, before exporting everything, will be to extract some of the most important values and metrics from our new curves. To do this, we'll move over to the `5a_extract_standard.R` script, located in the same `two_arm_workflow` folder. Note that, if we were working with a cure model or wanted to fit the data to splines, we could instead go to `5b_extract_cure.R` or `5c_extract_splines.R` respectively.

<br>

# Extracting Important Data from Survival Curves
##### Demo script: 5a_extract_standard.R

As with the curve fitting part of the analysis, we'll start by looking at the survival curves with individually fit covariates. The first set of values we're interested in are the AIC and BIC scores for each distribution. These are both indicators of how closely each distribution fits to its source data, with lower values being better. The `get_fit_comparison` function will generate a table of AIC and BIC scores for each distribution, along with a rank for each score. The only required input is an object generated by the `fit.models` function.

```{r}
### Goodness of fit statistics ------------------------------------------------
AIC_BIC_intv <- easysurv::get_fit_comparison(model_fits_intv)
AIC_BIC_comp <- easysurv::get_fit_comparison(model_fits_comp)

AIC_BIC_intv
AIC_BIC_comp
```

We can see from these tables that the "Weibull" curve seems to have the best statistical fit in both arms, as indicated by the lowest ranks and values in both AIC and BIC.

We will also want to have access to the values of the coefficients used for each distribution, so that we have the option to regenerate the curve elsewhere or so we can use these coefficients for some other analysis. If we ever want to complete a probabilistic sensitivity analysis with these curves, we'll also need the covariance matrices. The `get_results_table` generates a table with all of these values. The `parameter` and `Coef` columns identify and supply the value for the coefficients in each distribution of interest. The `cov_marker`, `v1`, `v2` and `v3` columns describe labels and columns for the covariance matrices. Single coefficient matrices will only use `v1`, where matrices for two or three coefficients will spill into `v2` and `v3` respectively.

```{r}
### Results tables ------------------------------------------------------------
Table_intv <- easysurv::get_results_table(model_fits_intv$models)
Table_comp <- easysurv::get_results_table(model_fits_comp$models)
```

Note that this function doesn't want the entire `fit.models` object, but rather the `$models` module within said object. For simplicity, let's just look at a few distributions from each table.

```{r}
print(Table_intv[1:6, ], width = 200)
print(Table_comp[1:6, ], width = 200)
```

Sometimes, all we want is a trace of the survival probability over time. We can do this using the `predict_fits` function. This creates a simple data frame with a time column and a column for the survival probability of each of our distributions of interest. The only required inputs are an object generated from the `model.fits` function and a vector of times for which we want the survival probabilities.

```{r}
### Model predictions ---------------------------------------------------------
predicts_intv <- easysurv::predict_fits(fits = model_fits_intv, t = times)
predicts_comp <- easysurv::predict_fits(fits = model_fits_comp, t = times)

print(predicts_intv[1:6, ], width = 200)
print(predicts_comp[1:6, ], width = 200)
```

We can also extract all these values from jointly fit survival curves. All we have to do is change the `model.fits` object that we feed into the functions.

```{r}
### Goodness of fit statistics ------------------------------------------------
AIC_BIC_both <- easysurv::get_fit_comparison(model_fits_both)
```


The AIC/BIC table output isn't structured any differently, so we don't need to look at it here. We'll look at the results table output though...

```{r}
### Results tables ------------------------------------------------------------
Table_both <- easysurv::get_results_table(model_fits_both$models)
print(Table_both[1:9, -4], width = 300)
```

Here, we can see that there is a new "treatment effect" parameter for each distribution, `as.factor(treatment)2`. This is the coefficient informing the effect of changing the `treatment` variable from 1 (male) to 2 (female). Since there is a new coefficient for every distribution, the covariance matrices are one row and one column larger as well.

Note that we've suppressed the `cov_marker` column for this output so the entire data frame fits in the output space.

When we use `predict_fits` to generate a survival probability trace from jointly fit survival curves, everything is pretty much the same, including the output structure. The only difference is that we need to specify which arm we want to generate the trace for in the `group` argument. This is because there are two treatment arms within the `model_fits_both` object and these will have different survival probabilities at each time point.

```{r}
### Model predictions ---------------------------------------------------------
predicts_both_intv <- easysurv::predict_fits(
  fits = model_fits_both, t = times, group = 2)
predicts_both_comp <- easysurv::predict_fits(
  fits = model_fits_both, t = times, group = 1)
```

With that, we have all the objects and plots that we need for export!

<br>

# Exporting results to Excel
##### Demo script: two_arm.R, 6a_output_standard.R

As mentioned, `easysurv` outputs objects with an expectation that exporting to Excel may be required. In this demo example, the `openxlsx` package is leveraged along with a pre-generated "template" Excel workbook. The script for exporting is over in `6a_output_standard.R` but for our sake, we don't need to explore it in detail. Instead, to run this script, we'll just go back to the main `two_arm.R` script and run it using the `source` function.

```{r, eval = F}
source(paste0(directory, "/6a_output_standard.R"))
```

If everything is run correctly, we should have an Excel sheet pop up on our screen with all of our curves, values and plots neatly organized. As mentioned, this workbook is saved to the "Downloads" folder, but we can easily save and rename it to wherever/whatever we want.

<br>

# Conclusion

And with that, we have a set of standard parametric model outputs in both R and Excel! Look out for future functionality and vignettes for mixture cure and spline modelling. In the mean time, if you encounter any bugs or issues, please reach out to the authors.
